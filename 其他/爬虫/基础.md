过年放假选择没有回家，加上疫情的缘故，有些时间就想着学爬虫玩玩。顺手写下了这几篇博文。

学爬虫之前需要掌握的Python基础：



安装环境，是学习编程的必要走第一步。我是让朋友咸糖帮我装的。新手很容易在这关上面卡住，或者耽误个一两天还是哪里不对用起来怪怪，挺打击积极性，也没有必要浪费时间在这上面，毫无意义。建议如果可以就请会的朋友帮忙弄一下，也就半小时的事。现代编译工具pycharm也是装环境必要的一步，最好把也装了，一劳永逸。

因为我之前一直用的是Java，所以对所有语言通用的知识点，比如变量、循环、数组什么的就没有专门学习。这篇文章主要列了一些Java和Python一些不同的地方，且仅限于语法。

是学习爬虫必须要掌握的Python基础语法。

### 注释 

单行注释采用 # 开头

多行注释三个单引号'''或三个双引号"""都行。

小技巧：pycharm选择文本注释的快捷键 ctrl+/

### 输出数组 

Java还需要调用数组的toString函数，Python直接输出就行了

` print(arr)`



### 字符串转换

#### 与整型的转换

非常简单...

字符串str转换成int: int_value = int(str_value)

int转换成字符串str: str_value = str(int_value)



#### 与数组的转换

##### 字符串转数组

利用split函数，这个函数的作用的把以参数（下面这个例子就是逗号）为分隔符切片，返回结果是一个数组，每个切片是数组一个元素。

```py
text = 'a,b,c'
text = text.split(',')
print(text)
#输出['a', 'b', 'c']
```

##### 数组转字符串

利用join函数，这个函数会以 string 作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串。

```py
arr = ['a', 'b', 'c']
str=''.join(arr)
print(str)
#输出abc
```



### 列表

就是Java中的ArrayList呗

#### 访问的列表的常见方式

| L[2]  | 'Taobao'             | 读取列表中第三个元素     |
| ----- | -------------------- | ------------------------ |
| L[-2] | 'Runoob'             | 读取列表中倒数第二个元素 |
| L[1:] | ['Runoob', 'Taobao'] | 从第二个元素开始截取列表 |



#### 列表的常用方法

```
list = []          
list.append('Google')   # 添加元素
del list1[2]						 # 删除下标为2的元素
print "list2[1:5]: ", list2[1:5]
```

虽然不是Python的技巧，但是编程超好用的小技巧，谁用谁知道。



## 输出网页的代码

http://www.biquge.tv 这个网址是一个盗版小说网站，接下来它将成为我们小试牛刀的对象。一因为它规模不大，没啥反爬措施。二因为它主要的内容是文字，控制台就能看到效果。

我们将**使用requests库，传入url作为参数，给它的get方法，返回结果就是原始网页啦**

下面这四行代码就可以输出这个网页的代码。如果出来的是乱码，需要根据html编码方式解码，编码一般在html的开头几行就有。

把它粘贴到你的pycharm，运行试试吧，这就是你爬虫的HelloWorld啦。╰(●’◡’●)╮

```
import requests

req = requests.get('https://www.biqukan.com/1_1094/')
req.encoding = 'gbk' #指定以gbk方式解码
print(req.text)
```



## 爬取代码中想要的内容

我们将用到一个新的库**BeautifulSoup**，**这个库可以帮我们把原始的代码过滤出我自己想要的内容。**

首先，将网页的代码作为参数传入BeautifulSoup(req.text, 'lxml')

prettify可以将网页的代码格式化输出，方便我们观察代码的结构。

输出看看，代码是不是换了一个种更清晰的格式呢(๑•̀ㅂ•́)و✧

```
import requests
from bs4 import BeautifulSoup

req = requests.get('https://www.biqukan.com/1_1094/')
req.encoding = 'gbk' #指定以gbk方式解码

html = BeautifulSoup(req.text, 'lxml')
print(html.prettify()) # 输出用soup格式化的html
```



输出了如下内容，现在我们需要观察我们想要的内容到底在哪。

```
 <div class="listmain">
   <dl>
    <dt>
     《一念永恒》最新章节列表
    </dt>
    <dd>
     <a href="/1_1094/22744709.html">
      《三寸人间》上架啦，已40万字，可以开宰啦~~
     </a>
    </dd>
    <dd>
     <a href="/1_1094/22744708.html">
      新书《三寸人间》发布！！！求收藏！！
     </a>
    </dd>
    <dd>
     <a href="/1_1094/17967679.html">
      第1314章 你的选择（终）
     </a>
    </dd>
    <dd>
     <a href="/1_1094/17967303.html">
      第1313章 青灯古庙伴一生
     </a>
 ......
```

根据原始代码可知，文章的内容存放在class为listmain的div里，所有的a标签就是我们想要的内容。

重头戏来了，怎么把它取出来呢，我们可以用正则（一种文本匹配规则），但正则对于新手并不友好，学习成本高。还可以使用**BeautifulSoup的findAll方法**，这是一个非常好用常用的方法，**它可以按照你要求提出所有符合条件的内容。**

比如arr = textS.findAll('a')，就是取出代码中所有的a标签。

比如div = soup.findAll('div', class_='listmain')，就是取出代码中所有class名为class_='listmain的div。

于是我们将class为listmain的div里，所有的a标签放进一个数组里，然后将遍历这个数组。

```
import requests
from bs4 import BeautifulSoup

req = requests.get('https://www.biqukan.com/1_1094/')
req.encoding = 'gbk' #指定以gbk方式解码

soup = BeautifulSoup(req.text, 'lxml')
print(soup.prettify()) # 输出用soup格式化的html

div = soup.findAll('div', class_='listmain')
textS = BeautifulSoup(str(div), 'lxml')
arr = textS.findAll('a')
for a in arr:
    print(a)

```





```
# -*- coding: UTF-8 -*-
import requests
from bs4 import BeautifulSoup

    req = requests.get(url)#获取这个url的html
    req.encoding = 'gbk' #如果出来的是乱码，需要根据html编码方式解码，编码一般在html的开头几行就有
    soup = BeautifulSoup(req.text, 'lxml')#用BeautifulSoup解析，就可以使用它好用的api，而不用自己写正则去匹配文本
    print(soup.prettify()) # 输出用soup格式化的html
    div = soup.findAll('div', class_='listmain')#findAll常用的一个方法，把所有class叫listmain的div放进一个数组里。类似的还有find，只找第一个。
    textS = BeautifulSoup(str(div), 'lxml')#可以嵌套解析
    a = textS.findAll('a')
    self.nums = len(a[11:])  # 从11个元素开始计数
    for each in a[11:]:# 从11个元素开始遍历
    	print(each)

```



像知乎这种登录才能浏览的网站，得带上http消息头才能爬取，只需要给requests.get多传一个header格式的字符串，可以直接拷贝下面的字符串。

```
headers = {
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
}
html = requests.get(url, headers)
```



有些网站是用 Ajax 动态加载的，就像知乎，你看第一页回答的时候是一个url，当用户往下滑到底的时候，会偷偷换下一页url。当然这一切用户是毫无感知的。但如果你直接拷贝url栏上的url，就只能只爬到第一页的ur。

所以我们应付 ajax 的方法也很简单，截获浏览器发给服务器的请求，然后分析出请求的规律，然后我们用爬虫伪装成浏览器不断向服务器发送请求，这样就可以获取源源不断的数据了。

一般就的url分页参数加倍。

在哪看请求:将开发者工具切换到 Network 窗口，但是有很多请求，而我们需要找到我们需要的url。为了方便看，可以先把它清空。

咋找

1. 过滤器中选XHR，请求的类型基本都是 XHR 这一类型的。
2. 点击请求切换到 Preview ，发现有实质性内容，这就是我们要找的请求url。

最后，分析请求头的格式（找规律），注意offset、index等字样的参数。



还有一件事， Ajax 动态加载拿到的是个json格式的， 用json加载器加载一下，把json内容塞进一个数组方便取。像这样json.loads(html.text)['data']。

```
# -*- coding: UTF-8 -*-
import requests, sys
import json

html = requests.get(url, headers=headers)
json_data = json.loads(html.text)['data']
comments = []
for item in json_data:
    comment = []
    comment.append("回答者:" + item['author']['name'])  # 姓名
    comment.append(item['content'])
    comments.append(comment)
  print(comments)
```



https://blog.csdn.net/wenxuhonghe/article/details/86515558





爬虫常见问题