## CAP理论

在 CAP 理论中 - 一个分布式系统不可能同时满足一致性、可用性和分区容错性，只能三选二。[](https://www.nowcoder.com/ta/coding-interviews)

### 一致性（Consistency）

在分布式环境中，一致性是指数据在多个副本之间能否保持一致的特性。一致性具有三个状态：弱一致性、强一致性、最终一致性。三者的区分是根据容忍程度来的。以数据更新的场景举例，强一致性要求后续所有的访问都可以获取到更新的数据；弱一致性则不要求所有的访问都能获取到更新后的数据，甚至所有的访问都获取不到也没关系；而最终一致性，则要求在一个**可控的时间范围**后，所有的访问都可以获取到更新的数据。

### 可用性（Availability）

系统提供的服务必须一直处于可用的状态，对于每个用户的操作总是在有限的时间内返回结果。

### 分区容错性（Partition tolerance）

分布式系统再遇到任何网络分区故障（指的应该就是脑裂）时，仍需要能够保证对外提供满足一致性和可用性的服务。

### 对于 CAP 三者之间的取舍

1. 放弃分区容错性，CA，意味着放弃了系统的可扩展性，所有的数据将被部署在一个节点上。这样子虽然能避免脑裂问题，但是这个做法跟我们设计分布式系统的初衷完全违背了。
2. 放弃可用性，CP，每个请求在服务器之间要保证强一致性，但是分区容错性的特性又说明了这是一个集群部署，所以数据需要在各副本之间同步完成之后才能继续对外提供服务，在这个同步期间，服务不可用。据说redis满足的就是CP（我个人的理解是，对于一个分布式的kv数据库，一致性是一定需要满足的，而分布式的特点又恰恰需要他满足分区容错性，因为redis一般都是集群搭建，可用性在三者的比重最小，所以选择了CP）
3. 放弃一致性，AP，其实这边说的是放弃了数据的强一致性，但是保留了数据的最终一致性。

其实对于一个分布式系统而言，**分区容错性是必然要保证的**，如果不保证分区容错性，那么应用就会成为单机服务，违背了分布式的初衷。所以我们需要根据对应的场景在C、A之间做抉择。



## 2PC

> 结婚是事务，神父是协调者，新郎和新娘是参与者：
>
> 神父问新娘：你愿意嫁给他吗？新娘说愿意。(phase1: request & reply)
>
> 神父问新郎：你愿意嫁给她吗？新郎说愿意。(phase1: request & reply)
>
> 神父对新娘说：那你戴上戒指就算结婚了，新娘戴上了，并回复说戴好了。（phase2: commit & ack）
>
> 神父对新郎说：那你戴上戒指就算结婚了，新郎戴上了，并回复说戴好了。（phase2: commit & ack）
>
> 
>
> 对于第一阶段，有任意一方失败，终止结婚。
>
> 对与第二阶段，
>
> 如果神父挂了，用备用的神父顶上。这段时间新郎新娘一直等待。
>
> 如果新郎挂了，新郎复活后，可以询问新郎她是否戴上了戒指，确定整个婚礼状态。
>
> 如果神父在对新郎说完后，自己挂了，然后新郎也挂了。用备用的神父顶上，但是不知道新郎的状态，不确定新郎是否戴上了戒指，【**系统状态不确定**】

 	

![success](http://www.hollischuang.com/wp-content/uploads/2015/12/success.png)



2PC的问题

1、**同步阻塞问题**。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

2、**单点故障**。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）

3、**数据不一致**。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。

4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。



## 3PC

>  班长要组织全班同学聚餐，由于大家毕业多年，所以要逐个打电话敲定时间，时间初定10.1日。然后开始逐个打电话。
>
> 班长：小A，我们想定在10.1号聚会，你有时间嘛？有时间你就说YES，没有你就说NO，然后我还会再去问其他人，具体时间地点我会再通知你，这段时间你可先去干你自己的事儿，不用一直等着我。（协调者询问事务是否可以执行，这一步不会锁定资源）
>
> 小A：好的，我有时间。（参与者反馈）
>
> 班长：小B，我们想定在10.1号聚会……不用一直等我。
>
> 班长收集完大家的时间情况了，一看大家都有时间，那么就再次通知大家。（协调者接收到所有YES指令）
>
> 班长：小A，我们确定了10.1号聚餐，你要把这一天的时间空出来，这一天你不能再安排其他的事儿了。然后我会逐个通知其他同学，通知完之后我会再来和你确认一下，还有啊，如果我没有特意给你打电话，你就10.1号那天来聚餐就行了。对了，你确定能来是吧？（协调者发送事务执行指令，这一步锁住资源。如果由于网络原因参与者在后面没有收到协调者的命令，他也会执行commit）
>
> 小A顺手在自己的日历上把10.1号这一天圈上了，然后跟班长说，我可以去。（参与者执行事务操作，反馈状态）
>
> 班长：小B，我们觉得了10.1号聚餐……你就10.1号那天来聚餐就行了。
>
> 班长通知完一圈之后。所有同学都跟他说：”我已经把10.1号这天空出来了”。于是，他在10.1号这一天又挨个打了一遍电话告诉他们：嘿，现在你们可以出门拉。。。。（协调者收到所有参与者的ACK响应，通知所有参与者执行事务的commit）
>
> 小A，小B：我已经出门拉。（执行commit操作，反馈状态）  



![3](http://www.hollischuang.com/wp-content/uploads/2015/12/3.png)



3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有`CanCommit`、`PreCommit`、`DoCommit`三个阶段。与两阶段提交不同的是，三阶段提交有两个改动点。

1、引入超时机制。同时在协调者和参与者中都引入超时机制。
2、在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。



```
在doCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ）
```



参考资料

> [新郎新娘例子来源](https://zhuanlan.zhihu.com/p/33041551)
>
> [图片来源，思路比较清晰](https://www.hollischuang.com/archives/681)
>
> [知乎关于两者区别的问题](https://www.zhihu.com/question/264701955)



## 一致性模型

### 弱一致性

- DNS
- Gossip通信协议

### 强一致性算法

#### 主从同步

主从同步复制

1. Master介绍写请求
2. Master复制日志到Slave
3. Master等待，直到所有从库返回

**问题：一个节点失败，Master 阻塞，导致整个集群不可用，保证了一致性，可用性却大大降低**

#### 多数派

每次写入都保证写入大于 N/2 个节点，每次读入保证从大于 N/2 个节点中读取。

**问题：在并发环境下，无法保证系统的正确性，顺序非常重要**

#### Basic Paxos

一共分布式的共识算法，有很多版本

##### 角色

- Proposer - 提议者，接受Client议题并想国会提出提议。像议员。
- Acceptor - 决策者，只有投票人数超过法定人数提议才会最终被接受，像国会。
- Learner - 提议接受者，做备份啥的，像记录员。
- Client - 议题生产者，像民众。

##### 步骤

1. Prepare

   propser 提出一个提案，编号为 N，此 N 大于这个 ProPser 之前提出的议案编号，请求 acceptor 的 quorum（法定人数）接受

2. Promise

   如果 N 大于此 acceptor 之前接受的任何提案编号则接受，否则拒绝

3. Accept

   如果达到了 quorum，propser 会发出 accept 请求，请求包含提案编号 N，以及提案内容

4. Accepted

   如果此 acceptor 在此期间没有收到任何编号大于 N 的提案，则接受此提案内容，否则忽略

##### 问题

难实现，效率低（两轮RPC）、活锁

#### Multi Paxos

基本流程：首先有一个 Propser 竞选 Leader，只要在 Leader 不宕机，以后都由这个 Leader 提出提案

减少角色，进一步简化。首先 Server 中有一结点竞选 Leader

##### Multi Paxos与raft

- **效果**：Raft 等价于  (multi-)Paxos；
- **效率**：Raft 与 Paxos 一样高效；
- **结构**：Raft 更易于理解 - 为了增强可理解性，Raft 将 leader 选举、日志复制和安全性等关键元素分离，并采用更强的一致性以减少必须考虑的状态的数量；
- **实现**：Raft 更易于实现 - Raft 算法的论文中提供了许多有利于实现的指引；
- **安全性**：Raft 还包括一个用于变更集群成员的新机制，它使用重叠的大多数（overlapping majorities）来保证安全性。

#### raft



相对于 Paxos的学术界模型，Raft 更偏向工业化，可以理解为 Raft 是更为简单的Multi Paxos。
Raft所需要关注的问题有三个

- Leader Election - leader 选举
- Log Replication - log 复制到其他节点
- Safety - 数据操作的安全（这里的安全指的是一致性问题）

Raft重定义了角色（一个节点可能在不同的时间有着不同的以下三种身份）

- Leader - 请求的处理者
- Follower - 请求的被动更新者，从Leader接受更新请求，然后写入本地日志文件
- Candidate - 如果Follower副本在一段时间内没有收到Leader副本的心跳，则判断Leader可能已经故障，此时启动选主过程，此时副本会变成Candidate状态，直到选主结束。

在每个节点上都会存在着一个计时器，时间随机在 150ms ～ 300ms 之间。以下的几种情况会重置 timeout

- 收到选举的请求
- 收到 leader 的心跳包（心跳包是可以携带数据的）

接下来我们来说说 Raft 中最重要的两个过程 – 选举和复制日志

- 选主
  - 正常情况下的选主
    - 这个情况会发生在服务刚启动时，假设我们有五个节点，那么此时就有五个 follower。
    - 在一个节点倒计时结束 (Timeout) 后，这个节点的状态变成 Candidate 开始选举，它给其他几个节点发送选举请求。
    - 其他四个节点都返回成功，这个节点的状态由 Candidate 变成了 Leader，并在每个一小段时间后，就给所有的 Follower 发送一个 Heartbeat 以保持所有节点的状态。
    - Follower 收到 Leader 的 Heartbeat 后重设 Timeout。这是最简单的选主情况，只要有超过一半的节点投支持票了，Candidate 才会被选举为 Leader，5个节点的情况下，3个节点 (包括 Candidate 本身) 投了支持就行。
  - Leader 出故障情况下的选主
    - 一开始已经有一个 Leader，所有节点正常运行。
    - Leader 出故障挂掉了，其他四个 Follower 将进行重新选主。4个节点的选主过程和5个节点的类似
    - 如果在选出一个新的 Leader 后，原来的 Leader 恢复了又重新加入了，这个时候怎么处理？在 Raft 里，第几轮选举是有记录的，重新加入的 Leader 是第一轮选举 (Term 1) 选出来的，而现在的 Leader 则是 Term 2，所有原来的 Leader 会自觉降级为 Follower。
  - 多个 Candidate 情况下的选主
    - 假设一开始有4个节点，都还是 Follower
    - 有两个 Follower 同时 Timeout，都变成了 Candidate 开始选举，分别给一个 Follower 发送了投票请求
    - 两个 Follower 分别返回了ok，这时两个 Candidate 都只有2票，要3票才能被选成 Leader。两个 Candidate 会分别给另外一个还没有给自己投票的 Follower 发送投票请求。
    - 但是因为 Follower 在这一轮选举中，都已经投完票了，所以都拒绝了他们的请求。
    - 所以在 Term 2 没有 Leader 被选出来。这时，两个节点的状态是 Candidate，两个是 Follower，但是他们的倒计时器仍然在运行，最先 Timeout 的那个节点会进行发起新一轮 Term 3 的投票，继续进行选举，直到leader产生。
    - 如果 Leader Heartbeat 的时间晚于另外一个 Candidate timeout 的时间，另外一个 Candidate 仍然会发送选举请求。两个 Follower 已经投完票了，拒绝了这个 Candidate 的投票请求。Leader 进行 Heartbeat，Candidate 收到后状态自动转为 Follower，完成选主。
- 复制日志
  - 正常情况下复制日志
    - 假设有三个节点。一开始，Leader 和 两个 Follower 都没有任何数据。
    - 客户端发送请求给 Leader，储存数据 “caitiezhu”，Leader 先将数据写在本地日志，这时候数据还是 Uncommitted。- Leader 给两个 Follower 发送 AppendEntries 请求，数据在 Follower 上没有冲突，则将数据暂时写在本地日志，Follower 的数据也还是 Uncommitted。Follower 将数据写到本地后，返回 OK。
    - Leader 接收反馈信息，只要收到的成功的返回数量超过半数 (包含Leader)，Leader 将数据 “caitiezhu 的状态改成 Committed。( 这个时候 Leader 就可以返回给客户端了)- Leader 再次给 Follower 发送 AppendEntries 请求，收到请求后，Follower 将本地日志里 Uncommitted 数据改成 Committed。这样就完成了一整个复制日志的过程，三个节点的数据是一致的.
  - 网络分区情况下进行复制日志
    - 一开始有 5 个节点处于同一网络状态下。
    - 网络分区将节点分成两边，一边有两个节点，一边三个节点
    - 两个节点这边已经有 Leader 了，来自客户端的数据 “caiyang” 通过 Leader 同步到 Follower。（这里指的是只有两个节点的分区）
    - 因为只有两个节点，少于3个节点，所以 “caiyang” 的状态仍是 Uncommitted。所以在这里，服务器会返回错误给客户端
    - 另外一个分区有三个节点，进行重新选主。客户端数据 “caigoudan” 发到新的 Leader，通过和上一个情况相似的复制过程，同步到另外两个 Follower。
    - 因为这个分区有3个节点，超过半数，所以数据 “caigoudan” 都 Commit 了。
    - 网络状态恢复，5个节点再次处于同一个网络状态下。但是这里出现了数据冲突 “caiyang” 和 “caigoudan“
    - 三个节点的 Leader 广播 AppendEntries
    - 两个节点分区的 Leader 自动降级为 Follower，因为这个分区的数据 “caiyang” 没有 Commit，返回给客户端的是错误，客户端知道请求没有成功，所以 Follower 在收到 AppendEntries 请求时，可以把 “caiyang“ 删除，然后同步 ”caigoudan”，通过这么一个过程，就完成了在网络分区情况下的复制日志，保证了数据的一致性。



#### ZAB

ZAB 协议是为分布式协调服务 Zookeeper 专门设计的一种支持崩溃恢复和原子广播协议。
工作流程基本于 Raft 相同，在一些名词的叫法上可能存在区别，比如zab将一个leader的周期称为epoch，而raft则称为term。在实现上，raft为保证日志连续性，心跳包发送方向为 leader -> follower，而zab则相反。
我们先来了解一下 zxid。
这个 zxid 是在 ZAB 协议中的事务编号。ZXID 是一个 64 位的数字，其中低 32 位可以看作是一个简单的递增的计数器，针对客户端的每一个事务请求，Leader 都会产生一个新的事务并对该计数器进行 + 1 操作。高 32 位记录的就是 leader id。
接下来我们再来了解一下 zab 最重要的两个模式 - 广播模式和恢复模式

- 广播模式 - 其实流程跟 raft 的请求提交差不多
  - leader 从客户端接收到一个请求
  - leader 生成一个新的事物并为这个事物生成一个唯一的zxid
  - leader 将这个事务发送给所有的 follower 节点
  - follow 节点将收到的事务请求加入到历史队列中（history queue），并发送ack给leader
  - 当 leader 收到大多数 follower（大于半数） 的ack消息，leader会发送commit请求
  - 当 follower 接收到 commit 请求时，会判断该事务的zxid是否比历史队列中的所有zxid都小，如果是，则提交，如果不是，则等待比它更小的zxid提交
- 恢复模式 - 恢复模式大致可以分为四个阶段：选举、发现、同步、广播
  - 当leader崩溃后，集群进入选举阶段，开始选举出潜在的新leader（一般为集群中拥有最大zxid的节点，为什么选择 zxid 最大，我个人的猜想是因为zxid越大，代表数据越完整，丢失越少）
  - 进入发现阶段，follower 与潜在的新 leader 进行沟通，如果超过法定人数，则潜在的新的leader将epoch加1，代表新的leader产生且可用
  - 新的 leader 将自己事务日志中 proposal 但未 COMMIT 的消息处理（commit）
  - 新的 leader 与 follower 建立先进先出的队列， 先将自身有而 follower 没有的 proposal 发送给 follower，再将这些 proposal 的 COMMIT 命令发送给 follower，以保证所有的 follower 都保存了所有的 proposal、所有的 follower 都处理了所有的消息
  - 集群恢复到广播模式，开始接收客户端的请求





#### BASE

BASE是对CAP中一致性和可用性权衡对结果（也证明了我在上面所说的，CAP一般在CA之间做取舍，手动滑稽）。其核心思想是即便无法做到强一致性，但是每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性。

- 基本可用（Basically Available） – 分布式系统出现不可预知的故障时，允许损失不分可用性
  - 响应时间上的损失 - 比如一个接口的响应时间从 0.2s 变成了 1s
  - 功能上的损失 - 看了网上的说法，我觉得其实就是一个服务降级的行为，将一部分的流量引入降级页面
- 软状态（Soft state）
  - 允许系统中的数据存在中间状态，并且不会影响系统整体的可用性
- 最终一致性（Eventually consistent）
  - 在一定的时间后，数据达成一致
  - 在工程实践中，最终一致性的五个变种
    1. 因果一致性 - 对于存在应该关系的操作，比如 A 更新数据之后通知了 B,但是没有通知 C，这就说明AB之间存在因果关系，而C没有，那么后续 B的操作都是基于 A的新值，而 C则没有这样子的限制
    2. 读己之所写 - A 更新一个数据后，后续的访问不会再访问到旧值
    3. 会话一致性 - 执行变更操作后，客户端在同一个会话中读取到的始终是新值
    4. 单调读一致性 - 一个进程从系统中读取一个数据的值后（称为新值），后续系统对该进程对访问不会返回旧值
    5. 单调写一致性 - 一个系统需要能够保证来自同一个进程对写操作能被顺序执行

### 分布式算法

上面说了那么多，现在我们来谈一下几种常见的分布式算法（因为我也只知道那么几种）

#### 

Paxos 是一种基于消息传递且具有高度容错特性的一致性算法，是目前工人的解决分布式一致性问题最有效的算法之一。拜占庭将军问题和paxos议会问题。

一个一致性算法需要保证以下的几点：

- 在这些被提出的提案中，只有一个会被选定
- 如果没有提案被提出，那么就不会有被选定的提案
- 当一个提案被选定后，进程应该可以获取被选定的提案信息

对于一致性来说，安全性需求如下：

- 只有被提出的提案才能被选定
- 只有一个值被选定
- 如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个

在讨论协议过程之前，我们先来讨论一下参与协议的几个角色

- Proposer - 提议者（
- Acceptor - 决策者
- Learner - 最终决策的学习者（我个人对于 Acceptor 和 Learner 的理解是有点类似于主从架构，Acceptor 作为主，Learner作为从）
- Client - 议题生产者

一个 Acceptor

- 这其实是一种最简单的实现方式，只有一个决策者，那么所有的提案都发给该决策者，相当于一个串型化的任务，但是单机应用的缺陷就是当该节点挂掉之后，整个系统就无法正常工作。

多个 Acceptor

- 使用多个 Acceptor 来避免单点问题，保证了高可用，但是相对的也引入了很多问题。Proposer 向一个 Acceptor 集合发送提案，集合中的每个 Acceptor 都可能会批准该提案，当有超过半数的 Acceptor 批准了这个提案的时候，我们就可以认为该提案被选定了。（在这个法则中存在着一个潜规则，就是 Acceptor 必须批准它收到的第一个提案）但是这个方案也存在着很多问题。举个栗子，比如有n个提议者和n个决策者，每个决策中跟每个提议者以及其提议一一对应，那么这个时候就很难仲裁出哪个提议是有效的。第二个问题就是，比如存在5个接受者和2个提议者，但是其中一个接受者挂了，那么每个提议者分配到两个接受者，此时也很难仲裁出结果。



- 决策议案编号
  - Prepare（预提交）
    - proposers（注意这里加了s，包括后续的所有提议者和决策者都是如此，因为为了保证高可用，我们需要维护一个集群） 提出一个议案，编号为 N，此 N 大于这个 proposers 之前提出的提案编号。
    - 如果这个 N 大于这个 acceptor 之前接收的所有提案编号，就接收，否则就拒绝。（注意这里我说的是acceptor，而不是acceptors，那么这个决策者集群中可能存在多个状态，假设存在两种状态的编号，n编号的有i个，n+1编号的有j个，那么 i + j 就是决策者的总数）
  - Commit（提交数据）- 相较于预提交的只提交编号，这一阶段提交的是编号+数据
    - accept - 如果达到了多数派，proposers会发出accept请求，此请求包含编号和数据（即提案内容），n + value
    - accepted - 如果acceptor在此间没有收到任何编号大于n的提案，则接收提案，否则忽略。如果成功接收提案，则反馈接收信息给 proposers，此时 learner 也要同步接收到的数据。

![avatar](https://img-blog.csdn.net/20180322144910257?watermark/2/text/Ly9ibG9nLmNzZG4ubmV0L2ZlbmcxMjM0NXpp/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

其实看到这里，我在想，basic paxos和2pc的流程好像是差不多的，只不过两个提交数据的阶段不一样而已。Basic Paxos 存在着许多问题，比如

- 活锁
  - 其实 Basic Paxos 存在着活锁问题。假设此时有两个 proposer，第一个 proposer 发出了编号为1的议案，阶段一执行完成，此时所有的决策者都被编号1标记。这个时候第二个 proposer 发出来编号为2的议案（因为编号是递增的），阶段一执行完成，此时所有的决策者都被标记为2。第一个 proposer 执行阶段二时发现编号被修改，那么他需要发送编号为3的议案，同理，第二个 proposer 也会发送编号为4的议案，循环往复，成为活锁。
  - 对于活锁的解决方法也不难，我们只需要在每次发送提案前等待随机秒即可。
- 难实现
- 效率低 - 2 轮rpc

#### MULTI PAXOS

因为 Basic paxos 存在着活锁和两轮 rpc 的问题。所以为了应对这个问题，我们接下来来看看 Multi Paxos 是如何解决的。
因为活锁的产生是因为有多个 proposer，那么我们的思路就是在集群中寻找一个 leader，使得只有一个 proposer 在真正意义上能提出议案。
接下来我用自己的理解和自己的话来说一下 Multi Paxos 到底做了哪些改进。

- leader 选举 - 这个步骤在 leader 有效期内只会存在一次
  - 各个 proposer 向 accepter 发送选举信息
  - accepter 响应选举信息
  - proposer集群接收到响应信息后，根据响应情况选举出leader
- 提案提交
  - 因为第一步已经选举出了leader，所以以后每次的client请求都是一次rpc，直接由leader提交即可

上面讲的可能有点抽象，结合图示的话会更加清晰
![avatar](https://s2.ax1x.com/2019/08/11/eXIg3j.jpg)









# 具体实现

## 分布式锁

多个系统的去访问共享资源的安全问题

### redis

#### NX普通分布式锁

```
SET resource_name my_random_value NX PX 30000
```

set key value

- `NX`：表示只有 `key` 不存在的时候才会设置成功。
- `PX 30000`：意思是 30s 后锁自动释放。

#### RedLock 算法

这个场景是假设有一个 redis cluster，有 5 个 redis master 实例。然后执行如下步骤获取一把锁：

1. 获取当前时间戳，单位是毫秒；
2. 跟上面类似，轮流尝试在每个 master 节点上创建锁，过期时间较短，一般就几十毫秒；
3. 尝试在**大多数节点**上建立一个锁，比如 5 个节点就要求是 3 个节点 `n / 2 + 1`；
4. 客户端计算建立好锁的时间，如果建立锁的时间小于超时时间，就算建立成功了；
5. 要是锁建立失败了，那么就依次之前建立过的锁删除；
6. 只要别人建立了一把分布式锁，你就得**不断轮询去尝试获取锁**。

### zookeeper

创建锁就是创建临时 znode，释放锁就是删除这个 znode，或者会话结束（比如用户关了页面）。当别的客户端来创建锁会失败，想获取的锁的客户端可以有两种等待方式

注册监听器**监听这个锁**，一旦释放掉就会通知客户端。

或者

监听**排在自己前面**的那个人创建的 node 上，一旦某个人释放了锁，排在自己后面的人就会被 zookeeper 给通知。

### redis 分布式锁和 zk 分布式锁的对比

**两者的区别：客户端宕机后是否会自动释放锁、是否需要主动询问锁状态。**

- redis 分布式锁，1.如果获取锁的客户端挂了，只能等待超时时间后释放锁。2.需要自己不断去尝试获取锁，比较消耗性能。
- zk 分布式锁，1.创建的是临时 znode，会话关闭znode 也没了，自动释放锁。2.注册个监听器即可，不需要不断主动尝试获取锁，性能开销较小。



## zookeeper集群

zookeeper的恢复模式  选举ZXID的节点+发现阶段法定人数+新纪元+数据同步+保证事务一致+恢复

广播模式+接受写请求广播模式类似两段 leader收写请求生成ZXID+发送给所有的follows节点+follower加入历史队列并发送ack+超过法定数量ack消息，leader会发送commit+follower判断该事务的ZXID是不是比历史队列中的任何事务的ZXID都小，如果是则提交，如果不是则等待比它更小的事务的commit

> [Zookeeper初级面试七小问。 ](http://www.sohu.com/a/312796774_120104204)
>
> [树哥的博文](https://caitiezhu.github.io/2019/08/08/%E5%88%86%E5%B8%83%E5%BC%8F%E7%AE%97%E6%B3%95/#more)