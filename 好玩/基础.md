基础

注释 #

多行''' 或者"""

pycharm快捷键 ctrl+/

字符串的转换

字符串str转换成int: int_value = int(str_value)

int转换成字符串str: str_value = str(int_value)

```py
>>> text = 'a,b,c'
>>> text = text.split(',')
>>> text
[ 'a', 'b', 'c' ]
```

Alternatively, you can use `eval()` if you trust the string to be safe:

```py
>>> text = 'a,b,c'
>>> text = eval('[' + text + ']')
```



```py
['a', 'b', 'c']
>>> ''.join(L)
'abc'
```

python输出数组 直接输出 print(arr)



列表

| L[2]  | 'Taobao'             | 读取列表中第三个元素     |
| ----- | -------------------- | ------------------------ |
| L[-2] | 'Runoob'             | 读取列表中倒数第二个元素 |
| L[1:] | ['Runoob', 'Taobao'] | 从第二个元素开始截取列表 |

```
 
list = []          ## 空列表
list.append('Google')   ## 使用 append() 添加元素v
del list1[2]
print "list2[1:5]: ", list2[1:5]
```





```
req = requests.get(url=self.target)
req.encoding = 'gbk' #如果出来的是乱码
soup = BeautifulSoup(req.text, 'lxml')
print(soup.prettify()) # 输出用soup格式化的html
div = soup.findAll('div', class_='listmain')
textS = BeautifulSoup(str(div), 'lxml')
a = textS.findAll('a')
self.nums = len(a[11:])  # 剔除不必要的章节，并统计章节数
for each in a[11:]:
    self.names.append(each.string)
    self.urls.append(self.server + each.get('href'))
```





```
# -*- coding: UTF-8 -*-
import requests
from bs4 import BeautifulSoup

    req = requests.get(url)#获取这个url的html
    req.encoding = 'gbk' #如果出来的是乱码，看看html的编码方式，然后按它的方式解码。显示正常就不必加上这句了
    soup = BeautifulSoup(req.text, 'lxml')#用BeautifulSoup解析，就可以使用它好用的api，而不用自己写正则去匹配文本
    print(soup.prettify()) # 输出用soup格式化的html
    div = soup.findAll('div', class_='listmain')#findAll常用的一个方法，把所有class叫listmain的div放进一个数组里。类似的还有find，只找第一个。
    textS = BeautifulSoup(str(div), 'lxml')#可以嵌套解析
    a = textS.findAll('a')
    self.nums = len(a[11:])  # 从11个元素开始计数
    for each in a[11:]:# 从11个元素开始遍历
    	print(each)

```



像知乎这种登录才能浏览的网站，得带上http消息头才能爬取，只需要给requests.get多传一个header格式的字符串，可以直接拷贝下面的字符串。

```
headers = {
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36',
}
html = requests.get(url, headers)
```



有些网站是用 Ajax 动态加载的，就像知乎，你看第一页回答的时候是一个url，当用户往下滑到底的时候，会偷偷换下一页url。当然这一切用户是毫无感知的。但如果你直接拷贝url栏上的url，就只能只爬到第一页的ur。

所以我们应付 ajax 的方法也很简单，截获浏览器发给服务器的请求，然后分析出请求的规律，然后我们用爬虫伪装成浏览器不断向服务器发送请求，这样就可以获取源源不断的数据了。

一般就的url分页参数加倍。

在哪看请求:将开发者工具切换到 Network 窗口，但是有很多请求，而我们需要找到我们需要的url。为了方便看，可以先把它清空。

咋找

1. 过滤器中选XHR，请求的类型基本都是 XHR 这一类型的。
2. 点击请求切换到 Preview ，发现有实质性内容，这就是我们要找的请求url。

最后，分析请求头的格式（找规律），注意offset、index等字样的参数。



还有一件事， Ajax 动态加载拿到的是个json格式的， 用json加载器加载一下，把json内容塞进一个数组方便取。像这样json.loads(html.text)['data']。

```
# -*- coding: UTF-8 -*-
import requests, sys
import json

html = requests.get(url, headers=headers)
json_data = json.loads(html.text)['data']
comments = []
for item in json_data:
    comment = []
    comment.append("回答者:" + item['author']['name'])  # 姓名
    comment.append(item['content'])
    comments.append(comment)
  print(comments)
```



https://blog.csdn.net/wenxuhonghe/article/details/86515558





爬虫常见问题